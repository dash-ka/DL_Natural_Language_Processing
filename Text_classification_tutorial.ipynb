{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive-Bayes classifier\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes classifiers are linear classifiers, the adjective *naive* comes from the assumption that the\n",
    "features in a dataset are mutually independent. Despite this unrealistic assumption of indipendence, Naive-Bayes shows to perform well in practice. Naive-Bayas classifier is a probabilistic model based on Bayes Theorem.\n",
    "\n",
    "$$ P(ω_j|\\vec{x_i}) = \\frac{P(\\vec{x_i}|ω_j)*P(ω_j)}{P(\\vec{x_i})} $$\n",
    "\n",
    "$ω_j$ is the notation for the class $j$, $j\\in \\{1,2,...,m\\}$ \n",
    "\n",
    " $P(ω_j|\\vec{x_i})$  is the posterior probability, defines the probability that document $i$ belongs to the class $j$ given its observed feature values $\\vec{x_i}$. \n",
    " \n",
    " $P(\\vec{x_i}|ω_j)$ is the class conditional probability of observing document $\\vec{x_i}$(i.e that specific combination of words) given that it belongs to class $ω_j$ .\n",
    " \n",
    "The objective function in Naive-Bayes is to maximize the\n",
    "posterior probability given the training data, in order to formulate the following decision rule:\n",
    "    $$ \\text{predicted class label} \\leftarrow \\text{arg max}_{j=1...,m} P(ω_j |\\vec{x_i})$$\n",
    "    \n",
    "    \n",
    "* One assumption that Bayes classifiers make is that the samples are i.i.d. That is, random variables are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another observation\n",
    "* An additional assumption of naive Bayes classifiers is the conditional independence of features. That is, a particular word does not influence the chance of encountering other words in the same document.\n",
    " Under this naive assumption, the class-conditional probabilities or (likelihoods) of a specific combination of features in a document can be directly estimated from the training data, as a product of the individual conditional probabilities.\n",
    "\n",
    "$$ P(\\vec{x} | ω_j ) = P(x_1 | ω_j ) · P(x_2 | ω_j ) · . . . · P(x_d | ω_j ) = \\prod_{i=1}^{d} P(x_i | ω_j )$$\n",
    "\n",
    "\n",
    "$  P(\\vec{x} | ω_j )$  means, how likely is it to observe this particular feature\n",
    "pattern, $\\vec{x}$, given that it belongs to class $ω_j$.  Pattern refers to a combination of features or words in a document. The maximum-likelihood estimate for the individual word in the feature vector is simply a relative frequency of that word in the $j$-th class.\n",
    "\n",
    "$$ \\hat{P}(x_i| ω_j ) = \\frac{N_{x_i,ω_j}}{N_{ω_j}}  \\text{,   i = (1, ..., d)}$$\n",
    "\n",
    "In order to avoid the problem of zero probabilities, an additional smoothing term can be added to the multinomial Bayes model. The most common variants of additive smoothing are the so-called Lidstone smoothing (α < 1) and Laplace\n",
    "smoothing (α = 1).\n",
    "\n",
    "$$ \\hat{P}(x_i| ω_j ) = \\frac{N_{x_i,ω_j}+α}{N_{ω_j}+αd}  \\text{,   i = (1, ..., d)}$$\n",
    "\n",
    "\n",
    "\n",
    "The class priors $P(ω_j)$, describe the general probability of encountering a particular\n",
    "class. It can be estimated from the training data (assuming that the training data is i.i.d. and offers a representative sample of the entire population), using the maximum-likelihood estimate approach:\n",
    "$$ \\hat{P}(ω_j) = \\frac{N_{ω_j}}{N} \\text{,  j = (1, ..., m)}$$\n",
    "\n",
    "\n",
    "The evidence $P(\\vec{x_i})$ is the probability of encountering a particular feature pattern, $\\vec{x_i}$, independent from the class label. Although the evidence term is required to accurately calculate the posterior probabilities, it can be removed from the decision rule, since it is merely a scaling factor. In case of two target classes, the decision rule: *\\\"Classify document $\\vec{x_i}$ as $ω_1$ if $P(ω_1 | \\vec{x_i}) > P(ω_2 |\\vec{x_i})$ else classify the sample as $ω_2$\\\"*, can be simplified as follows:\n",
    " \n",
    "$$\\frac{P(\\vec{x_i}| ω_1) · P(ω_1)}{\\vec{x_i}} > \\frac{P(\\vec{x_i}| ω_2) · P(ω_2)}{\\vec{x_i}} ∝ P(\\vec{x_i}| ω_1) · P(ω_1) > P(\\vec{x_i}| ω_2) · P(ω_2)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model\n",
    "\n",
    "In order to train our Bayes classifier, we need a way to represent text data in numerical form. __Vectorization__ is a process used to represent a textual document as a vector of numerical features, $\\vec{x_i}$. A commonly used model in Natural Language Processing for modeling textual data is the so-called __bag-of-words model__, or BOW for short. \n",
    "The BOW model first creates a vocabulary of unique terms encountered in the training set. The vocabulary stores a mapping between distinct words and integer indices. Indices are incrementally assigned to unique words during the vocabulary creation.\n",
    "The vocabulary is then used to construct feature vectors from individual documents. The vocabulary size $|V|$, i.e the number of unique words in the training data, determines the size of the vector representation for a single document. In the BOW model, every word in the vocabulary corresponds to a dimension in a feature vector. Thus, each document is represented as a $d$-dimensional feature vector, where $d = |V|$. The BOW model is based on word counts of the respective documents. So that the $k$-th dimension in the feature vector of a given document contains the count of how many times the word mapped to index $k$, occurs in that document.\n",
    "\n",
    "Depending on the probabilistic model used for the Naive-Bayes classifier (the Multinomial or Bernoulli model), the feature vectors can either hold binary counts (1 if the word occurs in a particular document, 0 otherwise) or absolute counts (the frequency of the $k$-th word in the $i$-th document).\n",
    "\n",
    "\n",
    "|              | every | state | has   | its   | own  |laws  | every| country|culture|\n",
    "| -------------|:-----:| -----:|------:|------:|-----:|-----:|-----:|-------:|------:|\n",
    "|$\\vec{x}_{D1}$  | 1     |  3    |  1    |   1   |  1   |   1  |   0  |   0    |   0   |\n",
    "|$\\vec{x}_{D2}$ | 0     |  0    |  2    |   1   |  1   |   0  |   1  |   2    |   1   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Bag-of-Words representation, the documents need to be preprocessed first. In particular, we need to:\n",
    "* tokenize each document, i.e. split the strings into tokens and assign each unique token a numeric id.\n",
    "* count the occurrences of tokens in each document.\n",
    "* normalize word counts to account for different document length, and downweight frequently occuring tokens with low discriminatory power.\n",
    "\n",
    "In order to do the first two steps, scikit-learn provides the ``CountVectorizer`` class from ``feature_extraction.text`` module. CountVectorizer object converts a collection of text documents to a matrix of token counts. Via ``fit_transform()`` method of CountVectorizer we learn the vocabulary, mapping unique words into integer ids, and return a document-term matrix.\n",
    "The values inside feature vectors represent raw term frequency, i.e. the number of occurences of a word in a document. Some words occur very frequently in a corpus of documents. In classification task we are looking for features that help discriminate between distinct document categories. Therefore, frequenctly occuring features that are common to multiple documents cannot provide useful information. In order to downweight those common words in feature vectors we compute tf-idf score for each word. Tf-Idf score is defined as the product of the term frequency and the \n",
    "inverse document frequency. ``TfidfTransformer`` from sklearn library computes idf scores from term counts matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\dashb\\Downloads\\movie_data.csv\", encoding=\"utf-8\", header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "    text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "    ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "df[\"review\"] = df[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input ndarray should be 1D array\n",
    "counts = cv.fit_transform(df.iloc[:,0].values)\n",
    "counts.toarray()[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer exposes several utility functions to process the text.\n",
    "The analyzer is used internally by CountVectorizer to build a dictionary of features and transform documents to feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor: a wonderful test phrase!\n",
      "Tokenizer: ['WONDERFUL', 'test', 'phrase']\n",
      "Analyzer: ['wonderful', 'test', 'phrase']\n"
     ]
    }
   ],
   "source": [
    "preprocessor = cv.build_preprocessor() # preprocess text before tokenization: strip_accents and lowercase\n",
    "tokenizer = cv.build_tokenizer() # splits a string into a sequence of tokens.\n",
    "analyzer = cv.build_analyzer() # {‘word’, ‘char’, ‘char_wb’} or callable, default=’word’\n",
    "print(\"Preprocessor:\", preprocessor(\"A WONDERFUL test phrase!\"))\n",
    "print(\"Tokenizer:\", tokenizer(\"A WONDERFUL test phrase!\"))\n",
    "print(\"Analyzer:\", analyzer(\"A WONDERFUL test phrase!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.06, 0.  , 0.  , 0.  , 0.06, 0.  , 0.05],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = transformer.fit_transform(counts) # Learn the idf vector and transform into tf-idf representation.\n",
    "tfidf.toarray()[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we may directly apply ``TfidfVectorizer``, which converts a collection of raw documents to a matrix of TF-IDF features. It is equivalent to ``CountVectorizer`` followed by ``TfidfTransformer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.06, 0.  , 0.  , 0.  , 0.06, 0.  , 0.05],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.iloc[:10,0].values).toarray()[:5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Naive-Bayes classifier using Bag-of-Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.iloc[:25000, 0].values\n",
    "X_test = df.iloc[25000:, 0].values\n",
    "y_train = df.iloc[:25000, 1].values\n",
    "y_test = df.iloc[25000:, 1].values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Pipeline`` object takes in input a list of (name, transformer/estimator objects) tuples that are chained with the last object an estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement ``fit`` and ``transform`` methods. The final estimator only needs to implement ``fit``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"vectorizer\", TfidfVectorizer()),\n",
    "                 (\"classifier\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "predicted = pipe.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Grid Search (Logistic Regression)\n",
    "``GridSearchCV`` performs an exhaustive search over specified parameter values for an estimator. GridSearchCV implements a “fit” and a “score” method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBLINEAR solver for logistic classifier can perform better than the default choice ('lbfgs') for relatively large datasets.\n",
    "pipe = Pipeline([(\"vectorizer\", TfidfVectorizer()),\n",
    "                 (\"classifier\", LogisticRegression(solver = \"liblinear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = lambda x: x.split()\n",
    "porter_tokenizer = lambda x: [stemmer.stem(t) for t in x.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each dictionary is a separate parameter setting for the same estimator\n",
    "# pipe.get_params() to see the available hyperparameters to optimize\n",
    "param_grid = [{\"vectorizer__tokenizer\":[tokenizer, porter_tokenizer],\n",
    "              \"classifier__C\":[1., 10.]}, \n",
    "             \n",
    "             {\"vectorizer__tokenizer\":[tokenizer],\n",
    "              \"vectorizer__norm\":[None],\n",
    "              \"vectorizer__use_idf\":[False],\n",
    "              \"classifier__C\":[1., 10.]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10.0,\n",
       " 'vectorizer__tokenizer': <function __main__.<lambda>(x)>}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean cross-validated score of the best_estimator\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8984"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cls = cv.best_estimator_\n",
    "best_cls.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "\n",
    "We create ``stream_docs`` generator to yield a stream of documents. ``get_batch`` function collects documents into mini.batches of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dashb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    \"Tokenize and preprocess the text and remove stop words.\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "    text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return list(filter(lambda w: w not in stop, text.split()))\n",
    "\n",
    "\n",
    "def stream_docs(path):\n",
    "    \"\"\"Reads and returns one review at a time\"\"\"\n",
    "    with open(path, encoding=\"utf-8\") as csv:\n",
    "        next(csv) #skip the hearder (\"review\", \"sentiment\")\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "\n",
    "def get_batch(data_streamer, size):\n",
    "    \"\"\"Construct mini-batches with aspecified size.\"\"\"\n",
    "    revs, labs = [],[]\n",
    "    for _ in range(size):\n",
    "        rev, lab = next(data_streamer)\n",
    "        revs.append(rev)\n",
    "        labs.append(lab)  \n",
    "    return revs, labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ``HashingVectorizer`` to vectorize our documents, since we are implementing online learning and cannot use the CountVectirizer, which requires to hold the entire vocabulary in memory, nor the TfidfVectorizer which keeps the document-term matrix in memory to compute the inverse document frequency. For HasingVectorizer we need to specify ``n_features`` sufficiently high to avoid hash collisions. Setting the ``loss`` parameter to \"log\" we are implementing logistic regression classifier. Via ``partial_fit()`` method of the ``SDGClassifier`` we are performing online learning on a stream of mini-batches, each consisting of 1,000 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(n_features=2**20, tokenizer=tokenizer, decode_error=\"ignore\")\n",
    "cls = SGDClassifier(loss=\"log\", random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = stream_docs(r\"C:\\Users\\dashb\\Downloads\\movie_data.csv\")\n",
    "classes = np.unique(y_train) # 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(45):\n",
    "    batch_x, batch_y = get_batch(data_stream, 1000)\n",
    "    feature_vecs = vect.transform(batch_x)\n",
    "    cls.partial_fit(feature_vecs, batch_y, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once completed the incremental learning process, we will use the last 5,000 documents to evaluate the performance \n",
    "of our model. The accuracy of the model is approximately 87 percent, slightly below the accuracy \n",
    "that we achieved in the previous section using the grid search for hyperparameter tuning. However, \n",
    "the online learning is very memory efficient, and it took less than a minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_batch(data_stream, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8686"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vect.transform(X_test)\n",
    "cls.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
